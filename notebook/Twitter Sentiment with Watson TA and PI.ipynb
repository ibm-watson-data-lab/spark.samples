{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Twitter Sentiment analysis with Watson Tone Analyzer and Watson Personality Insights\n",
    "\n",
    "<img style=\"max-width: 800px; padding: 25px 0px;\" src=\"https://ibm-cds-labs.github.io/spark.samples/Twitter%20Sentiment%20with%20Watson%20TA%20and%20PI%20architecture%20diagram.png\"/>\n",
    "\n",
    "In this notebook, we perform the following steps:  \n",
    "1. Install python-twitter and watson-developer-cloud modules\n",
    "2. Install the streaming Twitter jar using PixieDust packageManager\n",
    "3. Invoke the streaming Twitter app using the PixieDust Scala Bridge to get a DataFrame containing all the tweets enriched with Watson Tone Analyzer scores\n",
    "4. Create a new RDD that groups the tweets by author and concatenates all the associated tweets into one blob\n",
    "5. For each author and aggregated text, invoke the Watson Personality Insights to get the scores\n",
    "6. Visualize results using PixieDust display  \n",
    "\n",
    "## Learn more \n",
    "* [Watson Tone Analyzer](http://www.ibm.com/watson/developercloud/tone-analyzer.html)  \n",
    "* [Watson Personality Insights](http://www.ibm.com/watson/developercloud/personality-insights.html)  \n",
    "* [python-twitter](https://github.com/bear/python-twitter)  \n",
    "* [watson-developer-cloud](https://github.com/watson-developer-cloud)  \n",
    "* [PixieDust](https://github.com/ibm-cds-labs/pixiedust)\n",
    "* [Realtime Sentiment Analysis of Twitter Hashtags with Spark](https://developer.ibm.com/clouddataservices/2016/01/15/real-time-sentiment-analysis-of-twitter-hashtags-with-spark)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Install python-twitter and watson-developer-cloud\n",
    "If you haven't already installed the following modules, run these 2 cells:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!pip install --user python-twitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!pip install --user watson-developer-cloud"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Install latest pixiedust\n",
    "Make sure you are running the latest pixiedust version. After upgrading restart the kernel before continuing to the next cells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!pip install --upgrade --user pixiedust"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install the streaming Twitter jar in the notebook from the Github repo\n",
    "This jar file contains the Spark Streaming application (written in Scala) that connects to Twitter to fetch the tweets and send them to Watson Tone Analyzer for analysis. The resulting scores are then added to the tweets dataframe as separate columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pixiedust\n",
    "jarPath = \"https://github.com/ibm-cds-labs/spark.samples/raw/master/dist/streaming-twitter-assembly-1.6.jar\"\n",
    "pixiedust.installPackage(jarPath)\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>If PixieDust or the streaming Twitter jar were just installed or upgraded, <span style=\"color: red\">restart the kernel</span> before continuing.</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use Scala Bridge to run the command line version of the app\n",
    "Insert your credentials for Twitter, Watson Tone Analyzer, and Watson Personality Insights. Then run the following cell. \n",
    "[Read how to provision these services and get credentials](https://github.com/ibm-cds-labs/spark.samples/blob/master/notebook/Get%20Service%20Credentials%20for%20Twitter%20Sentiment%20with%20Watson%20TA%20and%20PI.md). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pixiedust\n",
    "\n",
    "sqlContext=SQLContext(sc)\n",
    "\n",
    "#Set up the twitter credentials, they will be used both in scala and python cells below\n",
    "consumerKey = \"XXXX\"\n",
    "consumerSecret = \"XXXX\"\n",
    "accessToken = \"XXXX\"\n",
    "accessTokenSecret = \"XXXX\"\n",
    "\n",
    "#Set up the Watson Personality insight credentials\n",
    "piUserName = \"XXXX\"\n",
    "piPassword = \"XXXX\"\n",
    "\n",
    "#Set up the Watson Tone Analyzer credentials\n",
    "taUserName = \"XXXX\"\n",
    "taPassword = \"XXXX\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%%scala\n",
    "val demo = com.ibm.cds.spark.samples.StreamingTwitter\n",
    "demo.setConfig(\"twitter4j.oauth.consumerKey\",consumerKey)\n",
    "demo.setConfig(\"twitter4j.oauth.consumerSecret\",consumerSecret)\n",
    "demo.setConfig(\"twitter4j.oauth.accessToken\",accessToken)\n",
    "demo.setConfig(\"twitter4j.oauth.accessTokenSecret\",accessTokenSecret)\n",
    "demo.setConfig(\"watson.tone.url\",\"https://gateway.watsonplatform.net/tone-analyzer/api\")\n",
    "demo.setConfig(\"watson.tone.password\",taPassword)\n",
    "demo.setConfig(\"watson.tone.username\",taUserName)\n",
    "\n",
    "import org.apache.spark.streaming._\n",
    "demo.startTwitterStreaming(sc, Seconds(30))  //Run the application for a limited time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create a tweets dataframe from the data fetched above and transfer it to Python\n",
    "Notice the __ prefix for each variable which is used to signal PixieDust that the variable needs to be transfered back to Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%scala\n",
    "val demo = com.ibm.cds.spark.samples.StreamingTwitter\n",
    "val (__sqlContext, __df) = demo.createTwitterDataFrames(sc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Group the tweets by author and userid\n",
    "This will be used later to fetch the last 200 tweets for each author"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as F\n",
    "usersDF = __df.groupby(\"author\", \"userid\").agg(F.avg(\"Anger\").alias(\"Anger\"), F.avg(\"Disgust\").alias(\"Disgust\"))\n",
    "usersDF.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set up the Twitter API from python-twitter module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import twitter\n",
    "api = twitter.Api(consumer_key=consumerKey,\n",
    "                  consumer_secret=consumerSecret,\n",
    "                  access_token_key=accessToken,\n",
    "                  access_token_secret=accessTokenSecret)\n",
    "\n",
    "#print(api.VerifyCredentials())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# For each author, fetch the last 200 tweets\n",
    "use flatMap to return a new RDD that contains a list of tuples composed of userid and tweets text: (userid, tweetText)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def getTweets(screenName):\n",
    "    statuses = api.GetUserTimeline(screen_name=screenName,\n",
    "                        since_id=None,\n",
    "                        max_id=None,\n",
    "                        count=200,\n",
    "                        include_rts=False,\n",
    "                        trim_user=False,\n",
    "                        exclude_replies=True)\n",
    "    return statuses\n",
    "\n",
    "usersWithTweetsRDD = usersDF.flatMap(lambda s: [(s.user.screen_name, s.text.encode('ascii', 'ignore')) for s in getTweets(s['userid'])])\n",
    "print(usersWithTweetsRDD.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Concatenate all the tweets for each user so we have enough words to send to Watson Personality Insights\n",
    "* Use map to create an RDD of key, value pair composed of userId and tweets \n",
    "* Use reduceByKey to group all record with same author and concatenate the tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "usersWithTweetsRDD2 = usersWithTweetsRDD.map(lambda s: (s[0], s[1])).reduceByKey(lambda s,t: s + '\\n' + t)\\\n",
    "    .filter(lambda s: len(re.findall(r'\\w+', s[1])) > 100 )\n",
    "print(usersWithTweetsRDD2.count())\n",
    "#usersWithTweetsRDD2.take(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Call Watson Personality Insights on the text for each author\n",
    "Watson Personality Insights requires at least 100 words from its lexicon to be available, which may not exist for each user. This is why the getPersonlityInsight helper function guards against exceptions from calling Watson PI. If an exception occurs, then an empty array is returned. Each record with empty array is filtered out of the resulting RDD.\n",
    "\n",
    "Note also that we use broadcast variables to propagate the userName and password to the cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *\n",
    "from watson_developer_cloud import PersonalityInsightsV3\n",
    "broadCastPIUsername = sc.broadcast(piUserName)\n",
    "broadCastPIPassword = sc.broadcast(piPassword)\n",
    "def getPersonalityInsight(text, schema=False):\n",
    "    personality_insights = PersonalityInsightsV3(\n",
    "          version='2016-10-20',\n",
    "          username=broadCastPIUsername.value,\n",
    "          password=broadCastPIPassword.value)\n",
    "    try:\n",
    "        p = personality_insights.profile(\n",
    "            text, content_type='text/plain',\n",
    "            raw_scores=True, consumption_preferences=True)\n",
    "\n",
    "        if schema:\n",
    "            return \\\n",
    "                [StructField(t['name'], FloatType()) for t in p[\"needs\"]] + \\\n",
    "                [StructField(t['name'], FloatType()) for t in p[\"values\"]] + \\\n",
    "                [StructField(t['name'], FloatType()) for t in p['personality' ]]\n",
    "        else:\n",
    "            return \\\n",
    "                [t['raw_score'] for t in p[\"needs\"]] + \\\n",
    "                [t['raw_score'] for t in p[\"values\"]] + \\\n",
    "                [t['raw_score'] for t in p['personality']]   \n",
    "    except:\n",
    "        return []\n",
    "\n",
    "usersWithPIRDD = usersWithTweetsRDD2.map(lambda s: [s[0]] + getPersonalityInsight(s[1])).filter(lambda s: len(s)>1)\n",
    "print(usersWithPIRDD.count())\n",
    "#usersWithPIRDD.take(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convert the RDD back to a DataFrame and call PixieDust display to visualize the results\n",
    "The schema is automatically created from introspecting a sample payload result from Watson Personality Insights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pixiedust": {
     "displayParams": {
      "aggregation": "SUM",
      "handlerId": "barChart",
      "keyFields": "userid",
      "showLegend": "true",
      "stacked": "false",
      "staticFigure": "false",
      "title": "Personality Insights",
      "valueFields": "Challenge,Closeness,Curiosity,Excitement"
     }
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#convert to dataframe\n",
    "schema = StructType(\n",
    "    [StructField('userid',StringType())] + getPersonalityInsight(usersWithTweetsRDD2.take(1)[0][1], schema=True)\n",
    ")\n",
    "\n",
    "usersWithPIDF = sqlContext.createDataFrame(\n",
    "    usersWithPIRDD, schema\n",
    ")\n",
    "\n",
    "usersWithPIDF.cache()\n",
    "display(usersWithPIDF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compare Twitter users Personality Insights scores with this year presidential candidates\n",
    "\n",
    "For a quick look on the difference in Personality Insights scores Spark provides a describe() function that computes stddev and mean values off the dataframe. Compare differences in the scores of twitter users and presidential candidates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "candidates = \"realDonaldTrump HillaryClinton\".split(\" \")\n",
    "candidatesRDD = sc.parallelize(candidates)\\\n",
    "    .flatMap(lambda s: [(t.user.screen_name, t.text.encode('ascii', 'ignore')) for t in getTweets(s)])\\\n",
    "    .map(lambda s: (s[0], s[1]))\\\n",
    "    .reduceByKey(lambda s,t: s + '\\n' + t)\\\n",
    "    .filter(lambda s: len(re.findall(r'\\w+', s[1])) > 100 )\\\n",
    "    .map(lambda s: [s[0]] + getPersonalityInsight(s[1]))\n",
    "\n",
    "candidatesPIDF = sqlContext.createDataFrame(\n",
    "   candidatesRDD, schema\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "c = candidatesPIDF.collect()\n",
    "broadCastTrumpPI = sc.broadcast(c[0][1:])\n",
    "broadCastHillaryPI = sc.broadcast(c[1][1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pixiedust": {
     "displayParams": {
      "handlerId": "dataframe"
     }
    }
   },
   "outputs": [],
   "source": [
    "display(candidatesPIDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "candidatesPIDF.select('userid','Emotional range','Agreeableness', 'Extraversion','Conscientiousness', 'Openness').show()\n",
    "\n",
    "usersWithPIDF.describe(['Emotional range']).show()\n",
    "usersWithPIDF.describe(['Agreeableness']).show()\n",
    "usersWithPIDF.describe(['Extraversion']).show()\n",
    "usersWithPIDF.describe(['Conscientiousness']).show()\n",
    "usersWithPIDF.describe(['Openness']).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculate Euclidean distance (norm) between each Twitter user and the presidential candidates using the Personality Insights scores\n",
    "\n",
    "Add the distances into 2 extra columns and display the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pixiedust": {
     "displayParams": {
      "aggregation": "COUNT",
      "handlerId": "barChart",
      "keyFields": "closerHillary",
      "showLegend": "true",
      "stacked": "true",
      "staticFigure": "false",
      "valueFields": "closerHillary"
     }
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from pyspark.sql.types import Row\n",
    "def addEuclideanDistance(s):\n",
    "    dict = s.asDict()\n",
    "    def getEuclideanDistance(a,b):\n",
    "        return np.linalg.norm(np.array(a) - np.array(b)).item()\n",
    "    dict[\"distDonaldTrump\"]=getEuclideanDistance(s[1:], broadCastTrumpPI.value)\n",
    "    dict[\"distHillary\"]=getEuclideanDistance(s[1:], broadCastHillaryPI.value)\n",
    "    dict[\"closerHillary\"] = \"Yes\" if dict[\"distHillary\"] < dict[\"distDonaldTrump\"] else \"No\"\n",
    "    return Row(**dict)\n",
    "\n",
    "#add euclidean distances to Trump and Hillary\n",
    "euclideanDF = sqlContext.createDataFrame(usersWithPIDF.map(lambda s: addEuclideanDistance(s)))\n",
    "\n",
    "#Reorder columns to have userid and distances first\n",
    "cols = euclideanDF.columns\n",
    "reorderCols = [\"userid\",\"distHillary\",\"distDonaldTrump\", \"closerHillary\"]\n",
    "euclideanDF = euclideanDF.select(reorderCols + [x for x in cols if x not in reorderCols])\n",
    "\n",
    "#PixieDust display. \n",
    "#To visualize the distribution, select the bar chart display, use closerHillary as key and value and aggregation=count\n",
    "display(euclideanDF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optional: do some extra data science on the tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pixiedust": {
     "displayParams": {
      "aggregation": "COUNT",
      "handlerId": "barChart",
      "keyFields": "Anger",
      "showLegend": "true",
      "stacked": "true",
      "staticFigure": "false",
      "valueFields": "Openness"
     }
    }
   },
   "outputs": [],
   "source": [
    "tweets=__df\n",
    "tweets.count()\n",
    "display(tweets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compute the sentiment distributions for tweets with scores greater than 60% and create matplotlib chart visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#create an array that will hold the count for each sentiment\n",
    "sentimentDistribution=[0] * 13\n",
    "#For each sentiment, run a sql query that counts the number of tweets for which the sentiment score is greater than 60%\n",
    "#Store the data in the array\n",
    "for i, sentiment in enumerate(tweets.columns[-13:]):\n",
    "    sentimentDistribution[i]=__sqlContext.sql(\"SELECT count(*) as sentCount FROM tweets where \" + sentiment + \" > 60\")\\\n",
    "        .collect()[0].sentCount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "ind=np.arange(13)\n",
    "width = 0.35\n",
    "bar = plt.bar(ind, sentimentDistribution, width, color='g', label = \"distributions\")\n",
    "\n",
    "params = plt.gcf()\n",
    "plSize = params.get_size_inches()\n",
    "params.set_size_inches( (plSize[0]*2.5, plSize[1]*2) )\n",
    "plt.ylabel('Tweet count')\n",
    "plt.xlabel('Tone')\n",
    "plt.title('Distribution of tweets by sentiments > 60%')\n",
    "plt.xticks(ind+width, tweets.columns[-13:])\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compute the top hashtags used in each tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from operator import add\n",
    "import re\n",
    "tagsRDD = tweets.flatMap( lambda t: re.split(\"\\s\", t.text))\\\n",
    "    .filter( lambda word: word.startswith(\"#\") )\\\n",
    "    .map( lambda word : (word, 1 ))\\\n",
    "    .reduceByKey(add, 10).map(lambda (a,b): (b,a)).sortByKey(False).map(lambda (a,b):(b,a))\n",
    "top10tags = tagsRDD.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "params = plt.gcf()\n",
    "plSize = params.get_size_inches()\n",
    "params.set_size_inches( (plSize[0]*2, plSize[1]*2) )\n",
    "\n",
    "labels = [i[0] for i in top10tags]\n",
    "sizes = [int(i[1]) for i in top10tags]\n",
    "colors = ['yellowgreen', 'gold', 'lightskyblue', 'lightcoral', \"beige\", \"paleturquoise\", \"pink\", \"lightyellow\", \"coral\"]\n",
    "\n",
    "plt.pie(sizes, labels=labels, colors=colors,autopct='%1.1f%%', shadow=True, startangle=90)\n",
    "\n",
    "plt.axis('equal')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compute the aggregate sentiment distribution for all the tweets that contain the top hashtags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cols = tweets.columns[-13:]\n",
    "def expand( t ):\n",
    "    ret = []\n",
    "    for s in [i[0] for i in top10tags]:\n",
    "        if ( s in t.text ):\n",
    "            for tone in cols:\n",
    "                ret += [s.replace(':','').replace('-','') + u\"-\" + unicode(tone) + \":\" + unicode(getattr(t, tone))]\n",
    "    return ret \n",
    "def makeList(l):\n",
    "    return l if isinstance(l, list) else [l]\n",
    "\n",
    "#Create RDD from tweets dataframe\n",
    "tagsRDD = tweets.map(lambda t: t )\n",
    "\n",
    "#Filter to only keep the entries that are in top10tags\n",
    "tagsRDD = tagsRDD.filter( lambda t: any(s in t.text for s in [i[0] for i in top10tags] ) )\n",
    "\n",
    "#Create a flatMap using the expand function defined above, this will be used to collect all the scores \n",
    "#for a particular tag with the following format: Tag-Tone-ToneScore\n",
    "tagsRDD = tagsRDD.flatMap( expand )\n",
    "\n",
    "#Create a map indexed by Tag-Tone keys \n",
    "tagsRDD = tagsRDD.map( lambda fullTag : (fullTag.split(\":\")[0], float( fullTag.split(\":\")[1]) ))\n",
    "\n",
    "#Call combineByKey to format the data as follow\n",
    "#Key=Tag-Tone\n",
    "#Value=(count, sum_of_all_score_for_this_tone)\n",
    "tagsRDD = tagsRDD.combineByKey((lambda x: (x,1)),\n",
    "                  (lambda x, y: (x[0] + y, x[1] + 1)),\n",
    "                  (lambda x, y: (x[0] + y[0], x[1] + y[1])))\n",
    "\n",
    "#ReIndex the map to have the key be the Tag and value be (Tone, Average_score) tuple\n",
    "#Key=Tag\n",
    "#Value=(Tone, average_score)\n",
    "tagsRDD = tagsRDD.map(lambda (key, ab): (key.split(\"-\")[0], (key.split(\"-\")[1], round(ab[0]/ab[1], 2))))\n",
    "\n",
    "#Reduce the map on the Tag key, value becomes a list of (Tone,average_score) tuples\n",
    "tagsRDD = tagsRDD.reduceByKey( lambda x, y : makeList(x) + makeList(y) )\n",
    "\n",
    "#Sort the (Tone,average_score) tuples alphabetically by Tone\n",
    "tagsRDD = tagsRDD.mapValues( lambda x : sorted(x) )\n",
    "\n",
    "#Format the data as expected by the plotting code in the next cell. \n",
    "#map the Values to a tuple as follow: ([list of tone], [list of average score])\n",
    "#e.g. #someTag:([u'Agreeableness', u'Analytical', u'Anger', u'Cheerfulness', u'Confident', u'Conscientiousness', u'Negative', u'Openness', u'Tentative'], [1.0, 0.0, 0.0, 1.0, 0.0, 0.48, 0.0, 0.02, 0.0])\n",
    "tagsRDD = tagsRDD.mapValues( lambda x : ([elt[0] for elt in x],[elt[1] for elt in x])  )\n",
    "\n",
    "#Use custom sort function to sort the entries by order of appearance in top10tags\n",
    "def customCompare( key ):\n",
    "    for (k,v) in top10tags:\n",
    "        if k == key:\n",
    "            return v\n",
    "    return 0\n",
    "tagsRDD = tagsRDD.sortByKey(ascending=False, numPartitions=None, keyfunc = customCompare)\n",
    "\n",
    "#Take the mean tone scores for the top 10 tags\n",
    "top10tagsMeanScores = tagsRDD.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "params = plt.gcf()\n",
    "plSize = params.get_size_inches()\n",
    "params.set_size_inches( (plSize[0]*3, plSize[1]*2) )\n",
    "\n",
    "top5tagsMeanScores = top10tagsMeanScores[:5]\n",
    "width = 0\n",
    "ind=np.arange(13)\n",
    "(a,b) = top5tagsMeanScores[0]\n",
    "labels=b[0]\n",
    "colors = [\"beige\", \"paleturquoise\", \"pink\", \"lightyellow\", \"coral\", \"lightgreen\", \"gainsboro\", \"aquamarine\",\"c\"]\n",
    "idx=0\n",
    "for key, value in top5tagsMeanScores:\n",
    "    plt.bar(ind + width, value[1], 0.15, color=colors[idx], label=key)\n",
    "    width += 0.15\n",
    "    idx += 1\n",
    "plt.xticks(ind+0.3, labels)\n",
    "plt.ylabel('AVERAGE SCORE')\n",
    "plt.xlabel('TONES')\n",
    "plt.title('Breakdown of top hashtags by sentiment tones')\n",
    "\n",
    "plt.legend(bbox_to_anchor=(0., 1.02, 1., .102), loc='center',ncol=5, mode=\"expand\", borderaxespad=0.)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optional: Use Twitter demo embedded app to run the same app with a UI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%scala\n",
    "val demo = com.ibm.cds.spark.samples.PixiedustStreamingTwitter\n",
    "demo.setConfig(\"twitter4j.oauth.consumerKey\",consumerKey)\n",
    "demo.setConfig(\"twitter4j.oauth.consumerSecret\",consumerSecret)\n",
    "demo.setConfig(\"twitter4j.oauth.accessToken\",accessToken)\n",
    "demo.setConfig(\"twitter4j.oauth.accessTokenSecret\",accessTokenSecret)\n",
    "demo.setConfig(\"watson.tone.url\",\"https://gateway.watsonplatform.net/tone-analyzer/api\")\n",
    "demo.setConfig(\"watson.tone.password\",taPassword)\n",
    "demo.setConfig(\"watson.tone.username\",taUserName)\n",
    "demo.setConfig(\"checkpointDir\", System.getProperty(\"user.home\") + \"/pixiedust/ssc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!pip install --upgrade --user pixiedust-twitterdemo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pixiedust": {
     "displayParams": {
      "handlerId": "twitterdemo"
     }
    }
   },
   "outputs": [],
   "source": [
    "from pixiedust_twitterdemo import *\n",
    "twitterDemo()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The embedded app has generated a DataFrame called __tweets. Let's use it to do some data science"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pixiedust": {
     "displayParams": {
      "handlerId": "dataframe"
     }
    }
   },
   "outputs": [],
   "source": [
    "display(__tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pixiedust": {
     "displayParams": {
      "aggregation": "COUNT",
      "handlerId": "barChart",
      "keyFields": "emotion",
      "showLegend": "true",
      "stacked": "true",
      "valueFields": "score"
     }
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import Row\n",
    "from pyspark.sql.types import *\n",
    "emotions=__tweets.columns[-13:]\n",
    "distrib = __tweets.flatMap(lambda t: [(x,t[x]) for x in emotions]).filter(lambda t: t[1]>60)\\\n",
    "    .toDF(StructType([StructField('emotion',StringType()),StructField('score',DoubleType())]))\n",
    "display(distrib)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "__tweets.registerTempTable(\"pixiedust_tweets\")\n",
    "#create an array that will hold the count for each sentiment\n",
    "sentimentDistribution=[0] * 13\n",
    "#For each sentiment, run a sql query that counts the number of tweets for which the sentiment score is greater than 60%\n",
    "#Store the data in the array\n",
    "for i, sentiment in enumerate(__tweets.columns[-13:]):\n",
    "    sentimentDistribution[i]=sqlContext.sql(\"SELECT count(*) as sentCount FROM pixiedust_tweets where \" + sentiment + \" > 60\")\\\n",
    "        .collect()[0].sentCount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "ind=np.arange(13)\n",
    "width = 0.35\n",
    "bar = plt.bar(ind, sentimentDistribution, width, color='g', label = \"distributions\")\n",
    "\n",
    "params = plt.gcf()\n",
    "plSize = params.get_size_inches()\n",
    "params.set_size_inches( (plSize[0]*2.5, plSize[1]*2) )\n",
    "plt.ylabel('Tweet count')\n",
    "plt.xlabel('Tone')\n",
    "plt.title('Distribution of tweets by sentiments > 60%')\n",
    "plt.xticks(ind+width, __tweets.columns[-13:])\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from operator import add\n",
    "import re\n",
    "tagsRDD = __tweets.flatMap( lambda t: re.split(\"\\s\", t.text))\\\n",
    "    .filter( lambda word: word.startswith(\"#\") )\\\n",
    "    .map( lambda word : (word, 1 ))\\\n",
    "    .reduceByKey(add, 10).map(lambda (a,b): (b,a)).sortByKey(False).map(lambda (a,b):(b,a))\n",
    "top10tags = tagsRDD.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "params = plt.gcf()\n",
    "plSize = params.get_size_inches()\n",
    "params.set_size_inches( (plSize[0]*2, plSize[1]*2) )\n",
    "\n",
    "labels = [i[0] for i in top10tags]\n",
    "sizes = [int(i[1]) for i in top10tags]\n",
    "colors = ['yellowgreen', 'gold', 'lightskyblue', 'lightcoral', \"beige\", \"paleturquoise\", \"pink\", \"lightyellow\", \"coral\"]\n",
    "\n",
    "plt.pie(sizes, labels=labels, colors=colors,autopct='%1.1f%%', shadow=True, startangle=90)\n",
    "\n",
    "plt.axis('equal')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cols = __tweets.columns[-13:]\n",
    "def expand( t ):\n",
    "    ret = []\n",
    "    for s in [i[0] for i in top10tags]:\n",
    "        if ( s in t.text ):\n",
    "            for tone in cols:\n",
    "                ret += [s.replace(':','').replace('-','') + u\"-\" + unicode(tone) + \":\" + unicode(getattr(t, tone))]\n",
    "    return ret \n",
    "def makeList(l):\n",
    "    return l if isinstance(l, list) else [l]\n",
    "\n",
    "#Create RDD from tweets dataframe\n",
    "tagsRDD = __tweets.map(lambda t: t )\n",
    "\n",
    "#Filter to only keep the entries that are in top10tags\n",
    "tagsRDD = tagsRDD.filter( lambda t: any(s in t.text for s in [i[0] for i in top10tags] ) )\n",
    "\n",
    "#Create a flatMap using the expand function defined above, this will be used to collect all the scores \n",
    "#for a particular tag with the following format: Tag-Tone-ToneScore\n",
    "tagsRDD = tagsRDD.flatMap( expand )\n",
    "\n",
    "#Create a map indexed by Tag-Tone keys \n",
    "tagsRDD = tagsRDD.map( lambda fullTag : (fullTag.split(\":\")[0], float( fullTag.split(\":\")[1]) ))\n",
    "\n",
    "#Call combineByKey to format the data as follow\n",
    "#Key=Tag-Tone\n",
    "#Value=(count, sum_of_all_score_for_this_tone)\n",
    "tagsRDD = tagsRDD.combineByKey((lambda x: (x,1)),\n",
    "                  (lambda x, y: (x[0] + y, x[1] + 1)),\n",
    "                  (lambda x, y: (x[0] + y[0], x[1] + y[1])))\n",
    "\n",
    "#ReIndex the map to have the key be the Tag and value be (Tone, Average_score) tuple\n",
    "#Key=Tag\n",
    "#Value=(Tone, average_score)\n",
    "tagsRDD = tagsRDD.map(lambda (key, ab): (key.split(\"-\")[0], (key.split(\"-\")[1], round(ab[0]/ab[1], 2))))\n",
    "\n",
    "#Reduce the map on the Tag key, value becomes a list of (Tone,average_score) tuples\n",
    "tagsRDD = tagsRDD.reduceByKey( lambda x, y : makeList(x) + makeList(y) )\n",
    "\n",
    "#Sort the (Tone,average_score) tuples alphabetically by Tone\n",
    "tagsRDD = tagsRDD.mapValues( lambda x : sorted(x) )\n",
    "\n",
    "#Format the data as expected by the plotting code in the next cell. \n",
    "#map the Values to a tuple as follow: ([list of tone], [list of average score])\n",
    "#e.g. #someTag:([u'Agreeableness', u'Analytical', u'Anger', u'Cheerfulness', u'Confident', u'Conscientiousness', u'Negative', u'Openness', u'Tentative'], [1.0, 0.0, 0.0, 1.0, 0.0, 0.48, 0.0, 0.02, 0.0])\n",
    "tagsRDD = tagsRDD.mapValues( lambda x : ([elt[0] for elt in x],[elt[1] for elt in x])  )\n",
    "\n",
    "#Use custom sort function to sort the entries by order of appearance in top10tags\n",
    "def customCompare( key ):\n",
    "    for (k,v) in top10tags:\n",
    "        if k == key:\n",
    "            return v\n",
    "    return 0\n",
    "tagsRDD = tagsRDD.sortByKey(ascending=False, numPartitions=None, keyfunc = customCompare)\n",
    "\n",
    "#Take the mean tone scores for the top 10 tags\n",
    "top10tagsMeanScores = tagsRDD.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "params = plt.gcf()\n",
    "plSize = params.get_size_inches()\n",
    "params.set_size_inches( (plSize[0]*3, plSize[1]*2) )\n",
    "\n",
    "top5tagsMeanScores = top10tagsMeanScores[:5]\n",
    "width = 0\n",
    "ind=np.arange(13)\n",
    "(a,b) = top5tagsMeanScores[0]\n",
    "labels=b[0]\n",
    "colors = [\"beige\", \"paleturquoise\", \"pink\", \"lightyellow\", \"coral\", \"lightgreen\", \"gainsboro\", \"aquamarine\",\"c\"]\n",
    "idx=0\n",
    "for key, value in top5tagsMeanScores:\n",
    "    plt.bar(ind + width, value[1], 0.15, color=colors[idx], label=key)\n",
    "    width += 0.15\n",
    "    idx += 1\n",
    "plt.xticks(ind+0.3, labels)\n",
    "plt.ylabel('AVERAGE SCORE')\n",
    "plt.xlabel('TONES')\n",
    "plt.title('Breakdown of top hashtags by sentiment tones')\n",
    "\n",
    "plt.legend(bbox_to_anchor=(0., 1.02, 1., .102), loc='center',ncol=5, mode=\"expand\", borderaxespad=0.)\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "pySpark (Spark 1.6.0) Python 2",
   "language": "python",
   "name": "pyspark1.6python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
